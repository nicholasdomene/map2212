%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Problem Set/Assignment Template to be used by the
%% Food and Resource Economics Department - IFAS
%% University of Florida's graduates.
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Version 1.0 - November 2019
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Ariel Soto-Caro
%%  - asotocaro@ufl.edu
%%  - arielsotocaro@gmail.com
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}
\usepackage{design_ASC}

\setlength\parindent{0pt} %% Do not touch this

%% -----------------------------
%% TITLE
%% -----------------------------
\title{EP 4 MAP 2212} %% Assignment Title

\author{Nicholas Gialluca Domene\\ %% Student name
N USP 8543417 - IME\\ %% Code and course name
Felipe de Moura Ferreira\\
N USP 98674702 - IME\\
\textsc{Universidade de SÃ£o Paulo}
}

\date{\today} %% Change "\today" by another date manually
%% -----------------------------
%% -----------------------------

%% %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\setlength{\droptitle}{-5em}    
%% %%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

\section*{Problem Statement}
\textbf{Taken from} https://www.youtube.com/watch?v=ZmaYDTLR8yw
\\ \\
- Consider the m-dimensional Multinomial statistical model,
with observations, x, prior information, y, and parameter Theta;

\begin{equation}
- x, y \in N^m, \Theta \in Sm = \{\Theta \in R^{+m} | \Theta'1 = 1\}, m = 3;
\end{equation}

The statistical model comprises:

- Posterior density potential, 
\begin{equation}
f(\theta|x, y) = \frac{1}{B(x + y)} \Pi_{I_i=1}^{m} \theta_i^{x_i + y_i - 1}
\end{equation}

- Cut-off set, 
\begin{equation}
T(v) = \{\Theta \in Sm | f(\theta|x, y) \leq v\}, v \geq 0
\end{equation}

- Truth function, 
\begin{equation}
W(v) = \int_{T(v)} f(\Theta|x, y) d\Theta
\end{equation}

W(v) is the posterior probability mass inside T(v), that is, 
the probability mass where the posterior potential, $f(\Theta|x, y)$,
does not exceed the threshold level v
\begin{equation}
- Dirichlet(\theta|a) = \frac{\Pi_{i=1}^m \theta_i^{a_i - 1}}{B(a)}, 
\end{equation}
\begin{equation}
B(a) = \frac{\Pi_{i=1}^m \Gamma(a_i)} {\Gamma(\sum_{i=1}^m a_i)}
\end{equation}

B(a) is the multivariate Beta function

- Set k cut-off points, $0 = v_0 < v_1 < v_2 < ... < v_k = sup f(\Theta)$

- Use a Gamma RNG to generate n points in Sm,
$\Theta_1, ..., \Theta_n$, distributed according to
the posterior density function

- Use the fraction of simulated points, $\Theta_t$, inside each
\"bin\", $v_{j-1} \leq f(\Theta_t) < v_j$, as an approximation of 
$W(v_j) - W(v_{j - 1})$

- Dynamically adjust the bin's borders, $v_j$, to get bins of approximately
equal weight, i.e., $W(t_j) - W(t_{j-1}) \approx \frac{1}{k}$

% %%%%%%%%%%%%%%%%%%%
\section*{Implementation}
% %%%%%%%%%%%%%%%%%%%

The algorithms were implemented using Python and the usage instructions can be found below. \\
The overall strategy was treated the implementation as an experiment that using the proposed conditions by the problem statement and a sufficiently large n (defined below), should yield an estimate to the integral W(v) with a Confidence Interval lesser or equal to 0.0005 with 95\% confidence.\\
We defined n through an algebraic manipulation of the Normal approximation of the Binomial distribution:
\begin{align}
Z_{score} & = \frac{mdd}{\sqrt{\frac{\sigma^{2}}{n}}}\nonumber\\
Z_{score}\cdot\frac{\sqrt{\sigma^{2}}}{\sqrt{n}} & = mdd\nonumber\\
\frac{Z_{score}\cdot\sqrt{\sigma^{2}}}{mdd} & = \sqrt{n}\nonumber\\
n &= \frac{Z_{score}^{2}\cdot\sigma^{2}}{mdd^{2}}
\end{align}

where $mdd$ is the ``\textit{Minimal Detectable Difference}'' which in this case is $0.0005$, meaning that in the worst case scenario it will $0.0005$ (if $\int_{0}^{1}f(x)dx=1$). Considering the worst case scenario:

\begin{eqnarray}
mmd = 0.0005,\quad Z_{score} = 1.65,\quad \sigma^{2} = 0.25
\end{eqnarray}

\begin{eqnarray}
n = \frac{1.65^{2} \cdot 0.25}{0.0005^{2}} = 2722500
\end{eqnarray}

\subsection*{How to run:}
1. start by initiating the EP4 class defined in \texttt{ep4\_class.py} inputting the vector \texttt{x} and \texttt{y}, both of dimension 3. In the \texttt{\_\_init\_\_} method, we will store both x and y vectors, a \texttt{alpha} vector that is the sum of each $x_i$ and $y_i$ for each $alpha_i$, the constant $B(x + y)$ of the denominator of the evaluating function $f(\Theta|x, y)$ and the \texttt{n} for generating \texttt{n} $\Theta$ observations of the simplex generated by the Dirichlet distribution.\\
\begin{lstlisting}
def __init__(self, x, y):
		self.x = x
		self.y = y
		self.n = 2722500
		self.alpha = [x[i] + y[i] for i in range(len(x))]

		numerator = 1
		for i in range(len(self.alpha)):
			numerator *= math.gamma(self.alpha[i])
		B_x_y = numerator / math.gamma(sum(self.alpha))

		self.denominator_constant = B_x_y
\end{lstlisting}
2 run the method \texttt{generate\_theta} that will generate the \texttt{n} $\Theta$ observations through the Dirichlet distribution with parameters \texttt{alpha} and store then into the variable \texttt{thetas}.\\
\begin{lstlisting}
def generate_theta(self):
		thetas = np.random.dirichlet(self.alpha, self.n)
		self.thetas = thetas
\end{lstlisting}
3 execute the method \texttt{order\_f\_thetas} that will create a list \texttt{f\_thetas} with the values of $f(\Theta\x, y)$ for each $\Theta_i$ and sort that list in ascending manner. The method will store the ordered list of $f(\Theta)$ values, the minimum and the maximum value for $f(\Theta)$ in the generated observations. This runs in $O(n \log n)$ time, n being the number of $\Theta$ observations generated.
\begin{lstlisting}
def order_f_thetas(self):
		f_thetas = [self.f(theta) for theta in self.thetas]
		f_thetas.sort()

		self.ordered_f_thetas = f_thetas
		self.min_f = f_thetas[0] #min value of f_thetas since it is ordered
		self.sup_f = f_thetas[-1] #max value of f_thetas since it is ordered

\end{lstlisting}
4 run the method \texttt{U(v)} passing \texttt{v} as the parameter to the desired output function \texttt{U}. The idea behind this method is:\\
Since we have an ordered list the the values of $f(\theta)$ for each $\theta$ in our sample space, then we need to find out how many theta observations have $f(\theta)$ below certain v. By finding the index where we would insert a new observation of value v in our  ordered list of $f(\theta)$, we use that index to determine how many observations are to the left (lower values). The number of observations whose $f(\theta)$ values are below v divided by the  total number of observations (n) is the estimate for W(v). This decreases exponentially the running time and gives a better estimate of W(v) than using bins since in the worst case scenario, it is exactly the same as using the proposed bins given that the is runs in $O(\log n)$ time. \\
So we are not using bins or the k-varible mentioned in the problem statement because this implementation is both faster and more accurate.
\begin{lstlisting}

	def U(self, v):

		if v > self.sup_f:
			return 1
		if v < self.min_f:
			return 0

		n = self.n
		i = bisect.bisect_left(self.ordered_f_thetas, v)
		return (i + 1)/n #index divided by total n points
\end{lstlisting}
In the \texttt{ep4.py} file, you will find a example of usage passing the \texttt{x} and \texttt{y} vectors and a few \texttt{test\_cases}, ready to be tested in the evaluation process:
\begin{lstlisting}
from ep4_class import EP4
x = [4, 6, 4]
y = [1, 2, 3]

test_cases = [0, 1, 0.5, 15, 20]

ep4 = EP4(x, y)

ep4.generate_theta()

ep4.order_f_thetas()

for test in test_cases:
	print("U(%s) = "%(test), ep4.U(test))
\end{lstlisting}

% %%%%%%%%%%%%%%%%%%%
\section*{Results}
% %%%%%%%%%%%%%%%%%%%
\begin{lstlisting}
[09:35:01] nicholas.domene ~/Documents/map2212/map2212/EP4 MAP2212 (main *% u=) $ python3 ep4.py 
Initiating...
U(0) =  0
U(1) =  0.040963820018365474
U(0.5) =  0.018735353535353536
U(15) =  0.8910321395775941
U(20) =  1
Time taken:  8.587098836898804  seconds
\end{lstlisting}
\section*{Conclusion}
We managed to create an algorithm that yields the desired accuracy (absolute error smaller than 0.0005 with 95\% confidence), an empirical precision until the 3rd decimal (many different runs yield results with that same precision, being different from the 4th decimal place and beyond) and with the fastest run time we heard of during benchmarks in the class discussion at the forum (Whatsapp) with an average of less than 10 seconds per test case given x and y vectors. \\
We attribute this results to the usage of the sorted list of $f(\theta)$ values and the binary search to figure out how many $\theta$ values yield $f(\theta)$ smaller or equal to the given threshold $v$ (fed into the U(v) evaluating function) and believe this is a good option to go forward.
\subsection*{Next steps}
To achieve greater precision, we believe it only takes a greater sample size of $\theta$ generated observations which should increase run time (approx) linearly. To test this, we ran an experiment with an n of 10,000,000 observations and it yielded the following results and run time:

\begin{lstlisting}
[09:35:14] nicholas.domene ~/Documents/map2212/map2212/EP4 MAP2212 (main *% u=) $ python3 ep4.py 
Initiating...
U(0) =  0
U(1) =  0.0411287
U(0.5) =  0.018692
U(15) =  0.8909363
U(20) =  1
Time taken:  33.3142511844635  seconds
\end{lstlisting}

Which, as expected, shows that by increasing the sample size by an order of 4, also increases the run time by approximately 4 times and greatly improves precision.
\end{document}